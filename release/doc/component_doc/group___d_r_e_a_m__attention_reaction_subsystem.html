<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Component Documentation: attentionReactionSubsystem</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="customdoxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="DREAM_223x59.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Component Documentation
   &#160;<span id="projectnumber">v0.01</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',false,false,'search.php','Search');
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('group___d_r_e_a_m__attention_reaction_subsystem.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">attentionReactionSubsystem</div>  </div>
</div><!--header-->
<div class="contents">

<p>Interface file for the attentionReaction Subsystem in the cognitive controller component.  
<a href="#details">More...</a></p>
<p>Class used to provided a simpler interface between the main code and the YARP layer.</p>
<h1><a class="anchor" id="lib_sec"></a>
Libraries</h1>
<p>YARP.</p>
<h1><a class="anchor" id="parameters_sec"></a>
Parameters</h1>
<p><b>Command-line Parameters </b></p>
<p>The following key-value pairs can be specified as command-line parameters by prefixing <code>&ndash;</code> to the key e.g. <code>&ndash;from</code> file.ini. The value part can be changed to suit your needs; the default values are shown below.</p>
<p><b>Configuration File Parameters </b></p>
<p>The following key-value pairs can be specified as parameters in the configuration file (they can also be specified as command-line parameters if you so wish). The value part can be changed to suit your needs; the default values are shown below.</p>
<dl class="section author"><dt>Author</dt><dd></dd></dl>
<p>Pablo Gómez, Vrije Universiteit Brussel, Belgium.</p>
<p>Copyright (C) 2016 DREAM Consortium</p>
<h1><a class="anchor" id="component_description"></a>
Component Description</h1>
<p>The attentionReaction Subsystem, as described in Deliverables D6.1 and D6.2, comprises the attention and the reactive subsystems.</p>
<p>The attention subsystem is a combination of perceptual attention, in which perceptual stimuli (reported by, for example, face detection or sound localization in work package WP4) that are particularly salient in the current context have to be selected, and attention emulation (from the Deliberative subsystem) to direct the robots attention and gaze. A gaze reaction triggered in social interactions is produced by the reactive subsystem. These inputs provide the robot with a locus of attention that it can use to organize its behavior. The attention subsystem is open for commands from the Self-Monitoring subsystem to overrule this locus of attention whenever the therapist considers it is needed.</p>
<p>For such purpose we have built a target selection algorithm adapted from Zaraki et al.'s model where a bottom-up attention model based on social features is presented. Some simplifications of such model have been done to adapt it to our context.</p>
<p>The Attention subsystem provides the gaze direction towards the Actuation subsystem.</p>
<p>The Reactive subsystem, as described in Deliverable D6.1, comprises three modules:</p>
<ul>
<li>Falling Reaction,</li>
<li>Social Reaction, and</li>
<li>Eye Blinking.</li>
</ul>
<p>The Falling Reaction module will be periodically checking the balance of the robot using the sensory information available. Changes in the balance may end up in a fall. In such case, a signal will be sent to the Self-Monitoring subsystem to interrupt any other running behavior, and a damage avoidance behavior that fits the situation will be triggered. Since the robot will be placed in a table and in case it falls it will be into the floor from certain height, there is no actual need to implement getting up behaviors. However, as the Nao robot includes such behaviors they will be taken into account. Additionally, the robot should include some speech acts to reduce the impact of such dramatic situation for the kid as saying that it has been a little bit clumsy or that it is tired today. Finally, back at its feet, the robot may apologize in order to engage the child back to the intervention or call the re-engagement module in the Deliberative subsystem and it will send a signal to the Self-Monitoring subsystem to restore the system functionality</p>
<p>The purpose of the social reaction module is to provide the appropriate social behavior in order to give the impression of the robot being socially alive. This module receives as input the sensory information where it is specified the childs social and affective state i.e. whether she/he is expressing an emotion or is performing a physical behavior (such as touching the robot unexpectedly). For each of these behaviors there should be a set of facial expressions and speech acts available to choose among them. Ideally it should randomize among them in order to look less predictable. Such reactive facial expressions and speech acts should be defined by the therapists and will be stored in the library of the Actuation subsystem. The functionality of this module can be switched on and off when needed through the Self-Monitoring subsystem.</p>
<p>Given the amount of studies made to model human blinking behavior we dont need to do our own but to use that one that best fits our requirements. Within the context in which DREAM will be applied, we need to recreate a blinking behavior mainly focused on the communicative behaviors and gaze shifts. For such reason, we consider that Ford et al.s model covers these needs and provide accurate data to implement their model. Ford et al. defines a model which considers multiple communicative facial behaviors and includes an individual blinking model for each of them. For each identified communicative behavior there is a probability to blink, a determined length, and so on. Moreover there is a passive behavior which simulates a physiological blink mechanism (for cleaning or humidifying the eye) that can be activated when no other blinking behavior has been triggered. To perform the blinking motion there is a blink morphology model which defines, based on statistics, if the blink is simple or multiple, full or half, its duration, etc.</p>
<p>The components that interact with this one have been developed as basic simulators that provide the expected output. For some of them, such expected output was provided through a GUI.</p>
<p>Such GUI has been used to simulate the sensory information of the robot as a first step in the implementation of this system. It should be replace by a sensory information simulator of an specific robot, i.e. Nao robot. But for the purpose of validating the attentionReaction Subsystem the GUI is an equally good option. On the other hand, this system uses actual actuators to show its outputs. This implementation has been done in a Nao Robot using different layers of Yarp (naoInterface component), keeping its platform-independent flavor. It also includes an Actuation subsystem simulator (actuationSimulator component) which receives the outputs of the attentionReaction Subsystem and provides the corresponding action primitives.</p>
<h1><a class="anchor" id="lib_sec"></a>
Libraries</h1>
<ul>
<li>YARP.</li>
</ul>
<h1><a class="anchor" id="parameters_sec"></a>
Parameters</h1>
<p><b>Command-line Parameters </b></p>
<p>The following key-value pairs can be specified as command-line parameters by prefixing <code>&ndash;</code> to the key e.g. <code>&ndash;from</code> file.ini. The value part can be changed to suit your needs; the default values are shown below.</p>
<ul>
<li><code>from</code> <code>attentionReactionSubsystem.ini</code> <ul>
<li>specifies the configuration file</li>
</ul>
</li>
<li><code>context</code> <code>components/attentionReactionSubsystem/config</code> <ul>
<li>specifies the sub-path from <code>$DREAM_ROOT/release</code> to the configuration file</li>
</ul>
</li>
<li><code>name</code> <code>attentionReactionSubsystem</code> <ul>
<li>specifies the name of the module (used to form the stem of module port names)</li>
</ul>
</li>
</ul>
<p><b>Configuration File Parameters </b></p>
<p>The following key-value pairs can be specified as parameters in the configuration file (they can also be specified as command-line parameters if you so wish). The value part can be changed to suit your needs; the default values are shown below.</p>
<table class="doxtable">
<tr>
<th align="left">Key </th><th align="left">Value  </th></tr>
<tr>
<td align="left">from </td><td align="left">attentionReactionSubsystem.ini </td></tr>
</table>
<p>specifies the configuration file</p>
<table class="doxtable">
<tr>
<th align="left">Key </th><th align="left">Value  </th></tr>
<tr>
<td align="left">context </td><td align="left">components/attentionReactionSubsystem/config </td></tr>
</table>
<p>specifies the sub-path from <code>$DREAM_ROOT/release/</code> to the configuration file</p>
<table class="doxtable">
<tr>
<th align="left">Key </th><th align="left">Value  </th></tr>
<tr>
<td align="left">name </td><td align="left">attentionReactionSubsystem </td></tr>
</table>
<p>specifies the name of the module (used to form the stem of module port names)</p>
<h1><a class="anchor" id="portsa_sec"></a>
Ports Accessed</h1>
<p><b>Input ports</b> </p><pre class="fragment">/attentionReactionSubsystem/checkMutualGaze:i
/attentionReactionSubsystem/getFaces:i
/attentionReactionSubsystem/getSoundDirection:i
/attentionReactionSubsystem/identifyFaceExpression:i 
/attentionReactionSubsystem/recognizeSpeech:i 
</pre><h1><a class="anchor" id="portsc_sec"></a>
Ports Created</h1>
<p><b>Input ports</b> </p><pre class="fragment">/attentionReactionSubsystem/actionFeedback:i
/attentionReactionSubsystem/attentionBias:i
/attentionReactionSubsystem/affectiveState:i
/attentionReactionSubsystem/attentionSwitchOff:i 
/attentionReactionSubsystem/reactionSwitchOff:i
/attentionReactionSubsystem/therapistGazeCommand:i
/attentionReactionSubsystem/robotSensors:i
</pre><p><b>Output ports</b></p>
<p>/attentionReactionSubsystem/eyeBlinking:o /attentionReactionSubsystem/fallingReaction:o /attentionReactionSubsystem/fallingReactionSpeech:o /attentionReactionSubsystem/socialFacialExpression:o /attentionReactionSubsystem/socialReaction:o /attentionReactionSubsystem/socialReactionSpeech:o /attentionReactionSubsystem/fallingInterruption:o</p>
<p><b>Port types </b></p>
<p>The functional specification only names the ports to be used to communicate with the module but doesn't say anything about the data transmitted on the ports. This is defined by the following code.</p>
<p>BufferedPort&lt;VectorOf&lt;double&gt;&gt; *getFacesIn, BufferedPort&lt;VectorOf&lt;double&gt;&gt; *getSoundDirectionIn, BufferedPort&lt;VectorOf&lt;double&gt;&gt; *attentionBiasIn, BufferedPort&lt;VectorOf&lt;double&gt;&gt; *therapistGazeCommandIn, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *checkMutualGazeIn, BufferedPort&lt;Bottle&gt; *recognizeSpeechIn, BufferedPort&lt;Bottle&gt; *affectiveStateIn, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *identifyFaceExpressionIn, BufferedPort&lt;Bottle&gt; *robotSensorsIn, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *attentionSwitchOffIn, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *reactionSwitchOffIn, BufferedPort&lt;Bottle&gt; *actionFeedbackIn, BufferedPort&lt;VectorOf&lt;double&gt;&gt; *elicitedAttentionOut, BufferedPort&lt;Bottle&gt; *eyeBlinkingOut, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *fallingInterruptionOut, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *fallingReactionOut, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *fallingReactionSpeechOut, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *socialReactionOut, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *socialReactionSpeechOut, BufferedPort&lt;VectorOf&lt;int&gt;&gt; *socialFacialExpressionOut</p>
<h1><a class="anchor" id="in_files_sec"></a>
Input Data Files</h1>
<p>None</p>
<h1><a class="anchor" id="out_data_sec"></a>
Output Data Files</h1>
<p>None</p>
<h1><a class="anchor" id="conf_file_sec"></a>
Configuration Files</h1>
<p><code>attentionReactionSubsystem.ini</code> </p>
<h1><a class="anchor" id="example_sec"></a>
Example Instantiation of the Component</h1>
<p><code>attentionReactionSubsystem &ndash;name attentionReactionSubsystem &ndash;context components/attentionReactionSubsystem/config &ndash;from attentionReactionSubsystem.ini </code></p>
<dl class="section author"><dt>Author</dt><dd></dd></dl>
<p>Pablo Gómez, Vrije Universiteit Brussel, Belgium.</p>
<p>Copyright (C) 2016 DREAM Consortium</p>
<p>Provide an output function sending data for each port, and an input function called when a message is received on a port.</p>
<h1><a class="anchor" id="lib_sec"></a>
Libraries</h1>
<p>YARP.</p>
<p><b>Port types </b></p>
<h1><a class="anchor" id="in_files_sec"></a>
Input Data Files</h1>
<p>None</p>
<h1><a class="anchor" id="out_data_sec"></a>
Output Data Files</h1>
<p>None</p>
<dl class="section author"><dt>Author</dt><dd></dd></dl>
<p>&lt;Pablo gomez="" esteban&gt;=""&gt;, &lt;VUB&gt;</p>
<p>Copyright (C) 2016 DREAM Consortium </p>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Mon Dec 11 2017 00:29:18 for Component Documentation by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
